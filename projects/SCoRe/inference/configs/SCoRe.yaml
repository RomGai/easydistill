model_name_or_path: ...
template: qwen
infer_backend: vllm  # choices: [huggingface, vllm, sglang]
trust_remote_code: true
skip_special_tokens: false

vllm_config:
  gpu_memory_utilization: 0.9
  tensor_parallel_size: 1
  dtype: bfloat16
  max_model_len: 8192

temperature: 0.6
top_p: 0.95
top_k: 20
repetition_penalty: 1.1
max_new_tokens: 4096
do_sample: true

